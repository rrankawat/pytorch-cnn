{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoAo9CiSWsx4ojv1d3SnC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrankawat/pytorch-cnn/blob/main/CIFAR_10_Static_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Training Static Quantization (PTQ)\n",
        "\n",
        "Post-Training Static Quantization (PTQ) is a technique that converts a pre-trained floating-point model to a lower-precision format, like an 8-bit integer, without requiring any retraining."
      ],
      "metadata": {
        "id": "VGIH_CHHgYqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "qWbXFPOUfsFE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.quantization\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os, time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk4MGWKkmNXG",
        "outputId": "b1b5ca36-2c88-472d-9af8-997cf72b90ad"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Base Model"
      ],
      "metadata": {
        "id": "-_B-8_hVhD3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)   # -> 16x32x32\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # -> 64x32x32\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1) # -> 64x32x32\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1) # -> 128x32x32\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.fc1 = nn.Linear(128*2*2, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 32 -> 16\n",
        "\n",
        "        # Block 2\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 16 -> 8\n",
        "\n",
        "        # Block 3\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 8 -> 4\n",
        "\n",
        "        # Block 4\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 4 -> 2\n",
        "\n",
        "        # Flatten\n",
        "        x = x.reshape(-1, 128*2*2)\n",
        "\n",
        "        # Fully connected\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z3RLG93jhF3z"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Quantization Wrapper"
      ],
      "metadata": {
        "id": "N9-0cJBozv7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedCIFAR(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super().__init__()\n",
        "        self.model_fp32 = model_fp32\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6f5s1VdIzxNn"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Load Trained Model Weights"
      ],
      "metadata": {
        "id": "z2XznSzomAFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = CIFARConvNet()\n",
        "model_fp32.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/model_cifar10.pth\"))\n",
        "model_fp32.eval()\n",
        "print(\"‚úÖ Trained model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWr0SjzLmA_g",
        "outputId": "908f65da-985b-438c-96ee-9d54df85d6d6"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trained model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap in quantization wrapper\n",
        "model_to_quant = QuantizedCIFAR(model_fp32)"
      ],
      "metadata": {
        "id": "-mWwl0M6z-fF"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CIFAR10 Data Loading"
      ],
      "metadata": {
        "id": "d8ot9xxkhbyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_data = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "kPrp3W_ihbf1"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Static Quantization Preparation"
      ],
      "metadata": {
        "id": "VLaLACDY0Y8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_quant.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "torch.quantization.prepare(model_to_quant, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6oUL7Id0aQQ",
        "outputId": "f8bb4fd7-138b-48a5-a334-29fc9e7fa2b1"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3731931240.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.quantization.prepare(model_to_quant, inplace=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedCIFAR(\n",
              "  (model_fp32): CIFARConvNet(\n",
              "    (conv1): Conv2d(\n",
              "      3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(\n",
              "      16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (conv2): Conv2d(\n",
              "      16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(\n",
              "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (conv3): Conv2d(\n",
              "      32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(\n",
              "      64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (conv4): Conv2d(\n",
              "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (bn4): BatchNorm2d(\n",
              "      128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (fc1): Linear(\n",
              "      in_features=512, out_features=256, bias=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (fc2): Linear(\n",
              "      in_features=256, out_features=10, bias=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.25, inplace=False)\n",
              "  )\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibrate using few batches\n",
        "with torch.no_grad():\n",
        "    for i, (images, _) in enumerate(test_loader):\n",
        "        model_to_quant(images)\n",
        "        if i > 10:\n",
        "            break\n",
        "\n",
        "quantized_model = torch.quantization.convert(model_to_quant.eval(), inplace=False)\n",
        "print(\"‚úÖ Static quantization done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-e9uKzc0dhN",
        "outputId": "e7ddb1f3-eb87-437b-eba8-12026ed67b3a"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-598525227.py:8: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.convert(model_to_quant.eval(), inplace=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Static quantization done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Accuracy Evaluation Function"
      ],
      "metadata": {
        "id": "IdoqolnA0icx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "acc_fp32 = evaluate(model_fp32, test_loader)\n",
        "acc_int8 = evaluate(quantized_model, test_loader)\n",
        "\n",
        "print(f\"üß† FP32 Accuracy: {acc_fp32:.2f}%\")\n",
        "print(f\"‚ö° INT8 Quantized Accuracy: {acc_int8:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69v2VC2t0jrm",
        "outputId": "0bdfd8c7-7edc-492c-d856-60cbfab8c914"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† FP32 Accuracy: 81.05%\n",
            "‚ö° INT8 Quantized Accuracy: 80.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Inference Time Comparison"
      ],
      "metadata": {
        "id": "b4dMGzFk0ojo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_time(model, loader, num_batches=20):\n",
        "    model.eval()\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, _) in enumerate(loader):\n",
        "            _ = model(images)\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "    return (time.time() - start) / num_batches\n",
        "\n",
        "t_fp32 = measure_inference_time(model_fp32, test_loader)\n",
        "t_int8 = measure_inference_time(quantized_model, test_loader)\n",
        "\n",
        "print(f\"‚è±Ô∏è FP32 Inference Time: {t_fp32:.4f} sec/batch\")\n",
        "print(f\"‚ö° INT8 Inference Time: {t_int8:.4f} sec/batch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7oOXGFT0p7h",
        "outputId": "cbeb95eb-6ff1-4019-ab54-084b0c86b4d5"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è±Ô∏è FP32 Inference Time: 0.1789 sec/batch\n",
            "‚ö° INT8 Inference Time: 0.0864 sec/batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Model Size Comparison"
      ],
      "metadata": {
        "id": "tqb1FbsV1TUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_fp32.state_dict(), \"model_fp32.pth\")\n",
        "torch.save(quantized_model.state_dict(), \"model_int8.pth\")\n",
        "\n",
        "fp32_size = os.path.getsize(\"model_fp32.pth\") / 1e6\n",
        "int8_size = os.path.getsize(\"model_int8.pth\") / 1e6\n",
        "\n",
        "print(f\"üì¶ FP32 Model Size: {fp32_size:.2f} MB\")\n",
        "print(f\"üì¶ INT8 Quantized Model Size: {int8_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib5vLrmO1UaX",
        "outputId": "4a2d2166-cb9a-41f6-f617-7d3f87112f9d"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ FP32 Model Size: 1.03 MB\n",
            "üì¶ INT8 Quantized Model Size: 0.27 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Save Quantized Model to Drive"
      ],
      "metadata": {
        "id": "O4L0gX191Yfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), \"/content/drive/My Drive/Colab Notebooks/model_cifar10_quantized.pth\")\n",
        "print(\"‚úÖ Quantized model saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swqtJCPu1Z7S",
        "outputId": "982a1fd0-65b0-44d4-937f-058337573c36"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Quantized model saved!\n"
          ]
        }
      ]
    }
  ]
}