{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMxJpE9V/LNnKxDi44WfvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrankawat/pytorch-cnn/blob/main/CIFAR_10_Static_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Training Static Quantization (PTQ)\n",
        "\n",
        "Post-Training Static Quantization (PTQ) is a technique that converts a pre-trained floating-point model to a lower-precision format, like an 8-bit integer, without requiring any retraining."
      ],
      "metadata": {
        "id": "VGIH_CHHgYqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qWbXFPOUfsFE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.quantization\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Base Model"
      ],
      "metadata": {
        "id": "-_B-8_hVhD3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)   # -> 16x32x32\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # -> 64x32x32\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1) # -> 64x32x32\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1) # -> 128x32x32\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.fc1 = nn.Linear(128*2*2, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 32 -> 16\n",
        "\n",
        "        # Block 2\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 16 -> 8\n",
        "\n",
        "        # Block 3\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 8 -> 4\n",
        "\n",
        "        # Block 4\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)  # 4 -> 2\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(-1, 128*2*2)\n",
        "\n",
        "        # Fully connected\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z3RLG93jhF3z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Quantization-Ready Model"
      ],
      "metadata": {
        "id": "2cnDF9FohPbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantCIFARConvNet(CIFARConvNet):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)  # Quantize input\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "        x = x.reshape(-1, 128 * 2 * 2) # Changed from view to reshape\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = self.dequant(x)  # Dequantize output\n",
        "        return x"
      ],
      "metadata": {
        "id": "rBXB01dPhMk7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CIFAR10 Data Loading"
      ],
      "metadata": {
        "id": "d8ot9xxkhbyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "kPrp3W_ihbf1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluate Model Accuracy"
      ],
      "metadata": {
        "id": "Aohtug2thvRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            predicted = torch.max(outputs.data, 1)[1]\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    return acc"
      ],
      "metadata": {
        "id": "0sUNc-lEhuar"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Prepare Model for Quantization"
      ],
      "metadata": {
        "id": "GNIVLt0GiIkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SOnqg_Wajib_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = QuantCIFARConvNet().to(device)\n",
        "model_fp32.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2TwNOmeiHb_",
        "outputId": "2a986ddc-b676-4358-a89a-32a72fa555a7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantCIFARConvNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuse Conv + BN + ReLU for better quantization performance\n",
        "# We skip explicit ReLU fusion here since it's inline (F.relu)\n",
        "torch.quantization.fuse_modules(model_fp32,\n",
        "    [['conv1', 'bn1'],\n",
        "     ['conv2', 'bn2'],\n",
        "     ['conv3', 'bn3'],\n",
        "     ['conv4', 'bn4']], inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05w6S7GLidiQ",
        "outputId": "182194ce-8a47-425a-dd7a-d9ed053ec3ba"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantCIFARConvNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): Identity()\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn2): Identity()\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn3): Identity()\n",
              "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn4): Identity()\n",
              "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set quantization configuration\n",
        "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')"
      ],
      "metadata": {
        "id": "85unCRJhiicT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model for static quantization\n",
        "torch.quantization.prepare(model_fp32, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3mSYYorikvs",
        "outputId": "b01caf8a-c6dc-4648-d9a5-42035a063893"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-834816604.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.quantization.prepare(model_fp32, inplace=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantCIFARConvNet(\n",
              "  (conv1): Conv2d(\n",
              "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (bn1): Identity()\n",
              "  (conv2): Conv2d(\n",
              "    16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (bn2): Identity()\n",
              "  (conv3): Conv2d(\n",
              "    32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (bn3): Identity()\n",
              "  (conv4): Conv2d(\n",
              "    64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (bn4): Identity()\n",
              "  (fc1): Linear(\n",
              "    in_features=512, out_features=256, bias=True\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (fc2): Linear(\n",
              "    in_features=256, out_features=10, bias=True\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Calibration Step"
      ],
      "metadata": {
        "id": "Gvtu0p9his09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calibrating the model...\")\n",
        "with torch.no_grad():\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        if i >= 10:  # 10 batches are enough for calibration\n",
        "            break\n",
        "        images = images.to(device)\n",
        "        model_fp32(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cia_upvWiuTM",
        "outputId": "af0cc82f-3765-49f8-8d43-43196f97a8b4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrating the model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Convert to Quantized Model"
      ],
      "metadata": {
        "id": "kghMJiRZi4R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.convert(model_fp32.eval(), inplace=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hL_2DvIi4AI",
        "outputId": "40f71130-3baa-4f5b-bf28-97216b3b66a5"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3576268019.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.convert(model_fp32.eval(), inplace=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluate Both Models (FP32 vs INT8)"
      ],
      "metadata": {
        "id": "-jNAeCi4i8Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For fair comparison, move models to CPU (quantized runs only on CPU)\n",
        "model_fp32_cpu = model_fp32.to('cpu')\n",
        "quantized_model_cpu = quantized_model.to('cpu')\n",
        "\n",
        "acc_fp32 = evaluate(model_fp32_cpu, test_loader, 'cpu')\n",
        "acc_quant = evaluate(quantized_model_cpu, test_loader, 'cpu')\n",
        "\n",
        "print(f\"\\nFP32 Model Accuracy: {acc_fp32:.2f}%\")\n",
        "print(f\"Quantized Model Accuracy: {acc_quant:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGGBxEy5i_LG",
        "outputId": "bc0e678d-3d88-4cdf-afd8-11ca4be2e1ea"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FP32 Model Accuracy: 10.00%\n",
            "Quantized Model Accuracy: 10.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Compare Model Sizes"
      ],
      "metadata": {
        "id": "5miGIBIekjiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_size(model, name):\n",
        "    torch.save(model.state_dict(), f\"{name}.pth\")\n",
        "    size = os.path.getsize(f\"{name}.pth\") / 1e6\n",
        "    print(f\"{name} size: {size:.2f} MB\")\n",
        "\n",
        "model_size(model_fp32_cpu, \"original_fp32\")\n",
        "model_size(quantized_model_cpu, \"quantized_int8\")\n",
        "\n",
        "print(\"\\nQuantization complete ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GpCkgrEklJR",
        "outputId": "f21659cf-3a73-43c9-df20-0a09655fd142"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_fp32 size: 1.00 MB\n",
            "quantized_int8 size: 0.25 MB\n",
            "\n",
            "Quantization complete ✅\n"
          ]
        }
      ]
    }
  ]
}